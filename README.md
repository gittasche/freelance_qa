# Тестовое задание по разработке сервиса QA по данным о фрилансерах
Ссылка на ТЗ: https://docs.google.com/document/d/1nyUe4Gpwp25GgO3XbdCMbse0nezWftt2QSuibVFNQ50/edit?tab=t.0#heading=h.cqdoqjf4z3c

### Стек
* langchain для работы агента
* qwen2.5:b7 в качестве тестовой модели
* ollama в качестве сервера тестовой модели
* SQLite - тестовая БД, но поддерживается любая БД совместимая с SQLAlchemy

### Инструкция по запуску (можно через uv, если удобнее):

`pip install .` - установка зависимостей и cli

`freelance-qa init-db` - создание тестовой SQLite БД

Далее необходимо иметь ollama сервер, например через docker-compose.yml (требуется nvidia container toolkit).

Если нужно использовать собственные модели, то их нужно добавить в конструктор FreelanceQA в файле `freelance_qa/cli/chat.py`.

```python
llm = ChatOpenAI(model="gpt-4o")
qa = FreelanceQA(llm=llm, llm_coder=llm)
```

`freelance-qa chat` - запуск чата

Тестовые вопросы лежат в `samples.json`.

Конфиг в `freelance_qa/config.py`

### Принцип работы
Так как данных может быть много и загрузить в лоб их нельзя, то нужно сохранить их в БД.
Для простоты используется SQLite, но это может быть Postgres или даже NoSQL, но желательно
что-то популярное т.к. запросы будет писать модель.

Имеются две модели: обычная и кодер. Кодер пишет запрос в БД, а обычная интерпретирует результат и даёт
ответ на человеческом языке.

Приложение получает вопрос и просит модель-кодер написать запрос на нужном диалекте. Модель имеет в контексте
полную информацию о БД: название и описание таблицы и полей. Также модели даются дополнительные указания по написанию
запроса.

Далее запрос исполняется, а из результатов собирается csv "строка". Если запрос исполнился с ошибкой, то
модель переписывает запрос и пытается снова.

В конце обычной модели передаются исходный вопрос, массив запросов и результатов,
на основе чего возвращает ответ.

### Критерии оценивания
Оценивать нужно по отдельности генерацию запросов и интерпретацию результатов. Модель может ответить правильно,
но SQL запрос будет не совсем верным. Например на вопрос "В америке больше заказов, чем в Европе?", модель-кодер напишет
запрос, который возвращает только числа, но не страны, к которым они относятся, а обычная модель - просто угадает.
Поэтому SQL запрос должен быть максимально полным.

### Что можно улучшить
* Надо смотреть эффективность модели на английском языке. Например, можно просить обычную модель перевести вопрос на английский язык и этот вопрос передавать кодеру.
Потребуется сделать все системные промпты на английском, а также переводить ответ обратно на исходный язык. Например qwen иногда выдавал в ответах китайские иероглифы,
т.е. он больше заточен под китайский язык.
* Можно сделать базу знаний "вопрос - SQL запрос" для модели кодера (few shot). Эти примеры можно прокидывать в контекст.
* Не на все вопросы можно ответить одним запросом, поэтому можно добавить модель, которая будет смотреть - достаточно ли данных для ответа на вопрос. Если их недостаточно,
то модель будет создавать уточняющий вопрос и отправлять его модели-кодеру. В случае с qwen, это работало плохо - модель расширяла вопрос, а не уточняла.
* Можно дать в системных промптах инструкции получше. qwen часто игнорировал инструкции, например на указание добавлять в SELECT все поля, перечисленные в GROUP BY, он не реагировал.
Возможно другие модели будут лучше учитывать это.
